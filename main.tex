\documentclass[
    a4paper,
    12pt,
    headinclude=true,
    BCOR=10mm,
    %toc=bibnumbered
]{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{microtype}
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily,
    literate={Ö}{{\"O}}1 {Ä}{{\"A}}1 {Ü}{{\"U}}1 {ß}{{\ss}}1 {ü}{{\"u}}1 {ä}{{\"a}}1 {ö}{{\"o}}1
}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage[german]{babel}
\usepackage{setspace}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}
\usepackage{svg}
\usepackage[backend=biber,style=authoryear,language=german]{biblatex}
\addbibresource{refs/refs.bib}
\ExecuteBibliographyOptions{date=year}
\renewbibmacro*{doi+eprint+url}{%
  \iftoggle{bbx:doi}
    {\printfield{doi}}
    {}%
  \newunit\newblock
  \iftoggle{bbx:eprint}
    {\usebibmacro{eprint}}
    {}%
  \newunit\newblock
  \iftoggle{bbx:url}
    {\iffieldundef{doi}
      {\usebibmacro{url+urldate}}
      {}}
    {}}
\renewbibmacro*{url+urldate}{%
  \printfield{url}%
  \ifentrytype{online}
    {\setunit*{\addspace}%
     \usebibmacro{urldate}}
    {\ifentrytype{misc}
      {\iffieldundef{doi}
        {\setunit*{\addspace}%
         \usebibmacro{urldate}}
        {}}
      {}}}
\DefineBibliographyStrings{german}{
    andothers = {et\addabbrvspace al\adddot},
    and = {und},
}

\usepackage[acronym]{glossaries}
\makeglossaries
\newacronym{nft}{NFT}{Non-Fungible Token}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\today}
\lhead{\namen}
\cfoot{\thepage}

\onehalfspacing

% User-defined commands
\newcommand{\namen}{H. Ehrmann}

\begin{document}
\begin{titlepage}
    \centering
    \vspace*{1cm}
    \Large{\textbf{PLATZHALTER FÜR TITEL}}\\
    \vspace{1.5cm}
    \Large{Fachhochschule Potsdam}\\
    \vspace{0.5cm}
    \large{Informations- und Datenmanagement}\\
    \vspace{1.5cm}
    Holger Ehrmann\\
    Matr. Nr. 21766\\
    \vspace{1cm}
    D11 Informationsintegration\\
    Sommersemester 25\\
    Prof. Dr. Günther Neher
    \vfill
    \today
\end{titlepage}
\setcounter{tocdepth}{1}
\begingroup
\setstretch{1.2} % Setzt den Zeilenabstand im Inhaltsverzeichnis
{\small \tableofcontents}
\endgroup

\chapter{Einleitung}

Die Umsetzung des Datenintegrationsprozesses erfolgt in einer isolierten Python-Umgebung, die mit den Bibliotheken \texttt{pandas}, \texttt{numpy}, \texttt{openpyxl}, \texttt{fuzzywuzzy}, \texttt{sqlalchemy}, \texttt{mysqlclient}, \texttt{psycopg2-binary} und \texttt{nltk} ausgestattet ist.

Die Rohdaten liegen als CSV-Datei mit dem Feldtrenner \texttt{\_§\_} vor und werden mit \texttt{pandas.read\_csv} unter expliziter UTF-8-Kodierung eingelesen. Im Anschluss werden führende sowie abschließende Unterstriche aus den Spaltennamen entfernt und numerische Datentypen, etwa \texttt{jobid} sowie die Koordinaten \texttt{geo\_lat} und \texttt{geo\_lon}, korrekt konvertiert; das Datumsfeld \texttt{date} wird in ein standardisiertes Zeitformat überführt. Dabei wird geprüft, ob die Normalisierung der Spaltennamen zu Dubletten führt. Nicht konvertierbare \texttt{jobid}- und Datumswerte werden protokolliert, um potenzielle Datenqualitätsprobleme sichtbar zu machen. Diese Normalisierung schafft eine konsistente Basis für alle weiteren Verarbeitungsschritte.

Zunächst wird ein umfangreiches Data Profiling durchgeführt, das strukturelle und inhaltliche Anomalien der Ausgangsdaten sichtbar macht. Auf dieser Grundlage folgen schrittweise Bereinigungen und Normalisierungen, wobei \texttt{fuzzywuzzy} zur Erkennung und Zusammenführung von Dubletten eingesetzt wird. Für die Anreicherung der Daten mit regionalen Informationen werden externe Referenzdaten eingebunden und über \texttt{sqlalchemy} mit den bestehenden Datensätzen verknüpft. Abschließend ermöglichen Skripte zur Extraktion von Freitextinformationen die Überführung unstrukturierter Angaben in auswertbare Felder. Diese methodische Kette gewährleistet eine transparente und reproduzierbare Aufbereitung des Datensatzes.

Im Hauptfenster der Anwendung steht dafür ein Button \glqq Data Profiling\grqq{} zur Verfügung. Nach dem Einlesen der CSV-Datei öffnet ein Klick darauf ein übersichtliches Dashboard, das für jede Spalte Zeilenzahl, fehlende sowie eindeutige Werte, relative Häufigkeiten und häufige Fehlerindikatoren darstellt. Die Breite des Fensters orientiert sich automatisch an den Spaltenbreiten und nutzt maximal die verfügbare Bildschirmbreite, sodass horizontales Scrollen in der Regel entfällt. Auf diese Weise lassen sich Datenqualitätsprobleme frühzeitig erkennen und gezielt beheben.

Nach Abschluss der Bereinigung wird der Button \glqq Dubletten entfernen\grqq{} freigeschaltet. Dabei prüft ein Fuzzy-Matching mittels TF-IDF-Vektorisierung und Nearest-Neighbor-Suche auf Basis der Attribute \texttt{company}, \texttt{location}, \texttt{jobtype} und \texttt{jobdescription} auf inhaltliche Dubletten. Gefundene Duplikate werden aus dem Dataframe entfernt und in einem separaten Fenster \glqq Gefundene Dubletten\grqq{} tabellarisch dargestellt.

Zur Benennung der Fehlertypen wurde die Klassifikation von Naumann und Leser genutzt, die in den folgenden beiden Abschnitten beschrieben werden.

\section{Fehler auf Schemaebene}

\textbf{Unzulässige Werte} entstehen, wenn Datenwerte außerhalb der für ein Attribut definierten Domäne liegen. Ein typisches Beispiel wäre ein Geburtsdatum wie \texttt{32.1.1997}, bei dem der Tag außerhalb des gültigen Bereichs von 1--31 liegt. Solche Fehler lassen sich durch Wertebereich-Validierungen und Format-Prüfungen systematisch identifizieren.

\textbf{Verletzte Attributabhängigkeiten} treten auf, wenn vordefinierte Beziehungen zwischen Attributwerten nicht eingehalten werden. Dies zeigt sich beispielsweise, wenn das Alter einer Person mit 17 Jahren angegeben ist, das Geburtsdatum jedoch auf den 2.1.1970 datiert ist. Programmatisch erfordern solche Fehler Cross-Field-Validierungen zwischen abhängigen Datenfeldern.

\textbf{Eindeutigkeitsverletzungen} entstehen in Attributen, die als eindeutig gekennzeichnet sind, aber dennoch mehrfach vorkommende Werte enthalten. Dies betrifft insbesondere Primärschlüssel oder andere Unique-Constraints und lässt sich durch Duplikat-Erkennung in entsprechenden Spalten aufdecken.

\textbf{Verletzte referenzielle Integrität} liegt vor, wenn Fremdschlüssel-Werte in referenzierten Tabellen nicht existieren. Solche Inkonsistenzen entstehen häufig bei unvollständigen Datenmigrationen oder fehlerhaften Löschoperationen und erfordern Join-Operationen zur Validierung der Referenzen.

\section{Fehler auf Datenebene}

\textbf{Fehlende Werte} manifestieren sich als NULL-Werte in Situationen, wo eigentlich Informationen verfügbar sein sollten. Problematisch wird dies besonders bei manueller Dateneingabe, wo oft Dummy-Werte wie \texttt{AAA} oder \texttt{999-999} verwendet werden, um NULL-Constraints zu umgehen. Diese Dummy-Werte sind meist nicht eindeutig definiert und erschweren die Identifikation und Bereinigung erheblich.

\textbf{Schreibfehler} entstehen sowohl bei manueller Dateneingabe als auch bei automatischer Schrifterkennung. Typische Beispiele sind Ortsnamen wie \texttt{Babbelsberg} statt \texttt{Babelsberg}. Solche Fehler lassen sich durch Domänenexperten erkennen und mit Hilfe von Rechtschreibprüfungen oder Ähnlichkeitsmaßen wie der Levenshtein-Distanz systematisch aufspüren.

\textbf{Falsche Werte} entsprechen nicht den tatsächlichen Gegebenheiten der realen Welt, auch wenn sie syntaktisch korrekt erscheinen. Ein Beispiel wäre ein Filmtitel \texttt{Ben Hur} mit dem Produktionsjahr 1931, obwohl der Film tatsächlich 1959 produziert wurde. Diese Fehlerart ist besonders schwer zu erkennen, da sie externe Referenzdaten zur Validierung erfordert und oft erst durch Beobachtung der realen Welt überprüfbar wird.

\textbf{Falsche Referenzen} treten auf, wenn Verweise zwischen Datensätzen semantisch inkorrekt sind, obwohl sie syntaktisch gültig erscheinen. Dies kann bei automatischen Verknüpfungen entstehen, wenn Algorithmen falsche Zuordnungen zwischen Entitäten herstellen.

\textbf{Kryptische Werte} sind für Menschen schwer interpretierbare Codes oder Abkürzungen ohne entsprechende Dokumentation. Solche Werte entstehen oft in Legacy-Systemen oder bei unvollständiger Datenmigration, wo Codetabellen verloren gehen.

\textbf{Eingebettete Werte} bezeichnen Situationen, wo mehrere logische Informationen in einem einzigen Datenfeld gespeichert sind. Ein typisches Beispiel wäre ein Adressfeld, das Straße, Hausnummer und Postleitzahl in einem String kombiniert, was die separate Verarbeitung dieser Komponenten erschwert.

\textbf{Falsche Zuordnungen} entstehen, wenn Datenwerte in falschen Attributspalten gespeichert werden. Dies kann bei Import-Prozessen auftreten, wenn Spalten-Mappings fehlerhaft definiert sind oder Datenformate nicht korrekt interpretiert werden.

\textbf{Widersprüchliche Werte} liegen vor, wenn innerhalb desselben Datensatzes logisch unvereinbare Informationen existieren. Ein Beispiel wäre eine Person mit dem Geschlecht \texttt{männlich} und dem Titel \texttt{Frau}, was semantisch inkonsistent ist.

\textbf{Transpositionen} bezeichnen die Vertauschung von Werten zwischen verschiedenen Attributen eines Datensatzes. Typisch wäre die Vertauschung von Vor- und Nachname oder die falsche Zuordnung von Koordinaten zu Längen- und Breitengrad.

\textbf{Duplikate} sind mehrfache Repräsentationen desselben realen Objekts innerhalb einer Datenquelle. Diese können durch wiederholte Eingabe, fehlerhafte Import-Prozesse oder unvollständige Bereinigungsverfahren entstehen und beeinträchtigen die Datenqualität erheblich.

\textbf{Datenkonflikte} entstehen, wenn verschiedene Versionen derselben Information in einem Datensatz koexistieren, ohne dass klar ist, welche Version korrekt oder aktuell ist. Dies tritt häufig bei zeitlichen Datenaktualisierungen auf, wenn alte Werte nicht ordnungsgemäß überschrieben werden.



\newpage
\addcontentsline{toc}{chapter}{Eigenständigkeitserklärung}
\chapter*{Eigenständigkeitserklärung}
Ich erkläre, dass
\begin{itemize}
    \item ich die schriftliche Prüfungsleistung (Hausarbeit und sonstige schriftliche Ausarbeitungen im Rahmen eines Leistungsnachweises) oder den von mir verantworteten und namentlich kenntlich gemachten Teil im Rahmen einer Gruppenarbeit selbstständig verfasst habe,
    \item ich keine anderen als die angegebenen Hilfsmittel und Quellen benutzt habe,
    \item Teile der Arbeit oder die Arbeit an sich nicht an anderer Stelle als Prüfungsleistung vorgelegt wurde und
    \item die Passagen der Arbeit, die fremden Werken wörtlich oder sinngemäß entnommen sind, unter Angabe der Quellen und unter Beachtung der im Wissenschaftsbereich geltenden allgemeinen verwendeten Zitierregelungen gekennzeichnet sind.
\end{itemize}

Zugleich erkläre ich, dass ich die Prüfungsleistung vor der Abgabe der Leistung bei QIS angemeldet habe.

\vspace{2cm}

\noindent
Potsdam, den \today

\vspace{2cm}

\noindent
\rule{6cm}{0.4pt}\\
Unterschrift

\addcontentsline{toc}{chapter}{Literaturverzeichnis}
\printbibliography
\newpage
\appendix
\end{document}
